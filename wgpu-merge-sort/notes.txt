# 2023-05-13
- Had some weird errors with bind group layout.
  - Seemed to not work with layout: 'auto'.
  - Tried with manual bind group layout specification, that worked.
  - Then layout: 'auto' worked, not sure what changed.
- Automatic bind group layouts don't include unused vars.
  - Have to use it in the WGSL otherwise setting those vars in the bind group will fail as it won't expect it to be there.
- device.queue.submit() doesn't return a promise.
  - Have to await onSubmittedWorkDone() instead.
- Finished GPU merge sort.
  - GPU wasn't faster than CPU for lists up to ~10M elements.
  - See graph.png for snapshot.
    - CPU is blue, GPU is orange.
- UI
  - Added colour legend for CPU/WGPU.
  - Added liveliness indicator for main thread.
    - Surprisingly dead during the WPGU sorting.
      - Maybe code is all in copying data across.

# 2023-05-12
- Turns out you can't use mapAsync() to read data out of storage buffers, it has to be a special dedicated buffer with MAP_READ and COPY_DST usage flags so you can copy the data out to it from another buffer for reading by the CPU.
  - Much GPU memory management.

# 2023-05-07
- Should be able to do bottom up merge sort on the GPU using logn parallel compute passes.
- Can benchmark it against Array.sort() and compare results for different sized lists.
- High level algorithm:
  - Have unsorted list size n.
  - For i in range(ceil(log2(n))):
    - Window width = 2 ** (i + 1).
    - Spawn compute ceil(n / window width) times.
    - Each compute merges the two halves of its window width with bounds checking.
- Bounds checking might slow things down, can try padding with infinity and compare speed.
- What's the inplace merge algorithm?
  - Probably just swapping the pointer values if the RHS is less than the LHS pointer.
  - How to merge 1 3 5 7 with 2 4?
  - SO says it's possible but not fun and not great.
  - Probably instead have another n sized buffer to use as merge output and swap between the two each level.
- Alternatively use quicksort for inplace sorting?
  - Doesn't have nice divisions like merge sort has, resulting LHS and RHS are arbitrary portions of the original array.
  - Could communicate where the resulting bounds are each level.
  - This requires buffer swapping all over again probably, each level invocation generates two more.
  - Could use the one buffer and just keep growing the portion of it that gets used.
    - Complicated index tracking, similar to a flat heap scheme.
- Wrote some starter code.
- Started some GPU compute set up.
  - Learned about @binding() and @group().
  - @binding() is for the device.createBindGroup() binding numbers.
  - @group() is for the GPU*PassEncoder.setBindGroup()
